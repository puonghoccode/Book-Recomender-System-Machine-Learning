# Importing necessary library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dữ liệu
books = pd.read_csv('data/BX-Books.csv', sep=';', encoding='latin-1', on_bad_lines='skip')

# Kiểm tra dữ liệu
print(books.head())
print("Shape of books:", books.shape)


# Xem các cột hiện có
print("Columns:", books.columns.tolist())


# Chọn các cột cần thiết
books = books[['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher', 'Image-URL-L']]

# Đổi tên cột cho dễ dùng
books.rename(columns={
    'Book-Title': 'title',
    'Book-Author': 'author',
    'Year-Of-Publication': 'year',
    'Publisher': 'publisher',
    'Image-URL-L': 'image_url'
}, inplace=True)

books.head()


# Load dataframe users
users = pd.read_csv('data/BX-Users.csv', sep=';', encoding='latin-1', on_bad_lines='skip')

# Kiểm tra dữ liệu
print(users.head())
print("Shape of users:", users.shape)


# Đổi tên cột cho dễ dùng
users.rename(columns={
    'User-ID': 'user_id',
    'Location': 'location',
    'Age': 'age'
}, inplace=True)

users.head()


# Load dataframe ratings
ratings = pd.read_csv('data/BX-Book-Ratings.csv', sep=';', encoding='latin-1', on_bad_lines='skip')

# Kiểm tra dữ liệu
print(ratings.head())
print("Shape of ratings:", ratings.shape)


# Đổi tên cột cho dễ dùng
ratings.rename(columns={
    'User-ID': 'user_id',
    'Book-Rating': 'rating'
}, inplace=True)

ratings.head()


print(books.shape)
print(users.shape)
print(ratings.shape)


ratings['user_id'].value_counts()


ratings['user_id'].value_counts().shape


# store users had rated more than 200 books
x = ratings['user_id'].value_counts() > 200
x[x].shape


y= x[x].index
y


ratings = ratings[ratings['user_id'].isin(y)]

# Kiểm tra dữ liệu
print(ratings.head())
print("Shape of ratings:", ratings.shape)


# Now join ratings with books
ratings_with_books = ratings.merge(books, on='ISBN')

# Kiểm tra dữ liệu
print(ratings_with_books.head())
print("New ratings:", ratings_with_books.shape)


number_rating = ratings_with_books.groupby('title')['rating'].count().reset_index()
print(number_rating.head())


number_rating.rename(columns={'rating':'num_of_rating'},inplace=True)
print(number_rating.head())


final_rating = ratings_with_books.merge(number_rating, on='title')
# Kiểm tra dữ liệu
print(final_rating.head())
print("New ratings:", final_rating.shape)


# Take those books which got at least 50 rating of user
final_rating = final_rating[final_rating['num_of_rating'] >= 50]
# Kiểm tra dữ liệu
print(final_rating.head())
print("New ratings:", final_rating.shape)


# drop the duplicates
final_rating.drop_duplicates(['user_id','title'],inplace=True)
# Kiểm tra dữ liệu
print(final_rating.head())
print("New ratings:", final_rating.shape)


# Lets create a pivot table
book_pivot = final_rating.pivot_table(columns='user_id', index='title', values= 'rating')
print(book_pivot)


book_pivot.fillna(0, inplace=True)
print(book_pivot)


from scipy.sparse import csr_matrix
book_sparse = csr_matrix(book_pivot)
type(book_sparse)


# clustering algoritm (Nearest Neighbors)
from sklearn.neighbors import NearestNeighbors
model = NearestNeighbors(algorithm= 'brute')
model.fit(book_sparse)


distance, suggestion = model.kneighbors(book_pivot.iloc[237,:].values.reshape(1,-1), n_neighbors=6 )
print("distance", distance)
print("suggestion", suggestion)


book_pivot.iloc[241,:]


for i in range(len(suggestion)):
    print(book_pivot.index[suggestion[i]])


book_pivot.index[3]


#keeping books name
book_names = book_pivot.index
np.where(book_pivot.index == '4 Blondes')[0][0]


import pickle
pickle.dump(model,open('artifacts/model.pkl','wb'))
pickle.dump(book_names,open('artifacts/book_names.pkl','wb'))
pickle.dump(final_rating,open('artifacts/final_rating.pkl','wb'))
pickle.dump(book_pivot,open('artifacts/book_pivot.pkl','wb'))


def recommend_book(book_name):
    book_id = np.where(book_pivot.index == book_name)[0][0]
    distance, suggestion = model.kneighbors(book_pivot.iloc[book_id,:].values.reshape(1,-1), n_neighbors=6 )
    
    for i in range(len(suggestion)):
            books = book_pivot.index[suggestion[i]]
            for j in books:
                if j == book_name:
                    print(f"You searched '{book_name}'\n")
                    print("The suggestion books are: \n")
                else:
                    print(j)





# Import thư viện
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Tạo cột 'combined' để TF-IDF từ title + author
books['combined'] = books['title'] + " " + books['author']


# Dùng final_rating
books_tfidf = books[books['title'].isin(final_rating['title'].unique())].reset_index(drop=True)

# Xử lý NaN trước khi ghép
books['title'] = books['title'].fillna('')
books['author'] = books['author'].fillna('')

# Tạo cột combined
books['combined'] = books['title'] + " " + books['author']


# TF-IDF
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(books_tfidf['combined'])

# Now cosine_similarity will be much smaller
cosine_sim = cosine_similarity(tfidf_matrix)

print("TF-IDF Matrix shape:", tfidf_matrix.shape)
print("Cosine similarity matrix shape:", cosine_sim.shape)


def recommend_content(book_title, top_n=5):
    # Lấy index của sách cần tìm
    idx = books_tfidf[books_tfidf['title'] == book_title].index[0]

    
    # Tính similarity score
    sim_scores = list(enumerate(cosine_sim[idx]))
    
    # Sắp xếp giảm dần theo độ similarity
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    
    # Lấy top_n sách tương tự nhất (loại bỏ chính nó)
    sim_scores = sim_scores[1:top_n+1]
    
    # Lấy index sách tương tự
    book_indices = [i[0] for i in sim_scores]
    
    # Trả về danh sách tên sách
    return books['title'].iloc[book_indices]



print(recommend_content('4 Blondes'))





from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(book_pivot)

# Thêm cluster vào index:
book_clusters = pd.DataFrame({'title': book_pivot.index, 'cluster': clusters})
print(book_clusters.head())


import pickle
import os
pickle.dump(cosine_sim, open('artifacts/cosine_sim.pkl', 'wb'))
pickle.dump(books, open('artifacts/books.pkl', 'wb'))
pickle.dump(book_clusters, open('artifacts/book_clusters.pkl', 'wb'))


pickle.dump(books_tfidf, open('artifacts/books_tfidf.pkl', 'wb'))



